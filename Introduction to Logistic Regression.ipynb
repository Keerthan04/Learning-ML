{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1c04e1",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd52691",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47789f12",
   "metadata": {},
   "source": [
    "Introduction\n",
    "When an email lands in your inbox, how does your email service know whether it’s real or spam? This evaluation is made billions of times per day, and one possible method is logistic regression.\n",
    "\n",
    "Logistic regression is a supervised machine learning algorithm that predicts the probability, ranging from 0 to 1, of a datapoint belonging to a specific category, or class. These probabilities can then be used to assign, or classify, observations to the more probable group.\n",
    "\n",
    "For example, we could use a logistic regression model to predict the probability that an incoming email is spam. If that probability is greater than 0.5, we could automatically send it to a spam folder. This is called binary classification because there are only two groups (eg., spam or not spam).\n",
    "\n",
    "Some other examples of problems that we could solve using logistic regression:\n",
    "\n",
    "Disease identification — Is a tumor malignant?\n",
    "Customer conversion — Will a customer arriving on a sign-up page enroll in a service?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdacbe",
   "metadata": {},
   "source": [
    "## Linear Reg approach"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a89fd1a",
   "metadata": {},
   "source": [
    "Linear Regression Approach\n",
    "With the data from Codecademy University, we want to predict whether each student will pass their final exam. Recall that in linear regression, we fit a line of the following form to the data:\n",
    "    y=b_0+b_1x_1+b_2x_2+…+b_nx_n\n",
    "where\n",
    "\n",
    "y is the value we are trying to predict\n",
    "b_0 is the intercept of the regression line\n",
    "b_1, b_2, … b_n are the coefficients\n",
    "x_1, x_2, … x_n are the predictors (also sometimes called features)\n",
    "For our data, y is a binary variable, equal to either 1 (passing), or 0 (failing). We have only one predictor (x_1): num_hours_studied. Below we’ve fitted a linear regression model to our data and plotted the results. The best fit line is in red.\n",
    "\n",
    "Linear Regression Model on Exam Data\n",
    "We see that the linear model does not fit the data well. Our goal is to predict whether a student passes or fails; however, a best fit line allows predictions between negative and positive infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceafe86",
   "metadata": {},
   "source": [
    "## Logistic Reg approach"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb0c2e1a",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "We saw that predicted outcomes from a linear regression model range from negative to positive infinity. These predictions don’t really make sense for a classification problem. Step in logistic regression!\n",
    "\n",
    "To build a logistic regression model, we apply a logit link function to the left-hand side of our linear regression function. Remember the equation for a linear model looks like this:\n",
    "\n",
    "  y=b0+b1x1+b2x2+b3x3+...+bnxn\n",
    " \n",
    "When we apply the logit function, we get the following:\n",
    " ln(y/(1-y))=b0+b1x1+b2x2+b3x3+...+bnxn\n",
    "    \n",
    "For the Codecademy University example, this means that we are fitting the curve shown below to our data — instead of a line, like in linear regression:\n",
    "\n",
    "sigmoid function imposed on the plot of passing vs. hours studied\n",
    "\n",
    "Notice that the red line stays between 0 and 1 on the y-axis. It now makes sense to interpret this value as a probability of group membership; whereas that would have been non-sensical for regular linear regression.\n",
    "\n",
    "Note that this is a pretty nifty trick for adapting a linear regression model to solve classification problems! There are actually many other kinds of link functions that we can use for different adaptations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8adb6",
   "metadata": {},
   "source": [
    "## Log Odds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "085bde2c",
   "metadata": {},
   "source": [
    "Log-Odds\n",
    "So far, we’ve learned that the equation for a logistic regression model looks like this:\n",
    " ln(p/(1-p)) = b0 + b1x1 + b2x2+b3x3+...+bnxn\n",
    " \n",
    "Note that we’ve replaced y with the letter p because we are going to interpret it as a probability (eg., the probability of a student passing the exam). The whole left-hand side of this equation is called log-odds because it is the natural logarithm (ln) of odds (p/(1-p)). The right-hand side of this equation looks exactly like regular linear regression!\n",
    "\n",
    "In order to understand how this link function works, let’s dig into the interpretation of log-odds a little more. The odds of an event occurring is:\n",
    "\n",
    "odds=p/(1-p)=p(event)/p(no event)\n",
    " \n",
    "For example, suppose that the probability a student passes an exam is 0.7. That means the probability of failing is 1 - 0.7 = 0.3. Thus, the odds of passing are:\n",
    "\n",
    "odds=0.7/(1-0.7)=2.33\n",
    "\n",
    "This means that students are 2.33 times more likely to pass than to fail.\n",
    "\n",
    "Odds can only be a positive number. When we take the natural log of odds (the log odds), we transform the odds from a positive value to a number between negative and positive infinity — which is exactly what we need! The logit function (log odds) transforms a probability (which is a number between 0 and 1) into a continuous value that can be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7e4fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "-0.4054651081081643\n",
      "9.0\n",
      "2.1972245773362196\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculate odds_of_rain\n",
    "odds_of_rain = 0.4/0.6\n",
    "print(odds_of_rain)\n",
    "\n",
    "# Calculate log_odds_of_rain\n",
    "log_odds_of_rain=np.log(odds_of_rain)\n",
    "print(log_odds_of_rain)\n",
    "\n",
    "\n",
    "# Calculate odds_on_time\n",
    "odds_on_time=0.9/0.1\n",
    "print(odds_on_time)\n",
    "\n",
    "\n",
    "# Calculate log_odds_on_time\n",
    "log_odds_on_time=np.log(odds_on_time)\n",
    "print(log_odds_on_time)\n",
    "\n",
    "#odds when we calci it is always a positive number when we take log of it we get it to be between positive and negative infinite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f79dc4",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b45eaffb",
   "metadata": {},
   "source": [
    "Sigmoid Function\n",
    "Let’s return to the logistic regression equation and demonstrate how this works by fitting a model in sklearn. The equation is:\n",
    "    ln(p / (1 - p)) = b0 + b1x1+ b2x2+ b3x3+...+ bnxn\n",
    "Suppose that we want to fit a model that predicts whether a visitor to a website will make a purchase. We’ll use the number of minutes they spent on the site as a predictor. The following code fits the model:\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(min_on_site, purchase)\n",
    "\n",
    "Next, just like linear regression, we can use the right-hand side of our regression equation to make predictions for each of our original datapoints as follows:\n",
    "\n",
    "log_odds = model.intercept_ + model.coef_ * min_on_site (doing b+b1x1)b1and all is coeff b is intercept x1 and all is feature\n",
    "print(log_odds)\n",
    "\n",
    "Output:\n",
    "\n",
    "[[-3.28394203]\n",
    " [-1.46465328]\n",
    " [-0.02039445]\n",
    " [ 1.22317391]\n",
    " [ 2.18476234]]\n",
    "\n",
    "Notice that these predictions range from negative to positive infinity: these are log odds. In other words, for the first datapoint, we have:\n",
    "\n",
    "    ln(p / (1 - p)) = -3.28\n",
    "We can turn log odds into a probability as follows:\n",
    "    ln(p / (1 - p)) = -3.28\n",
    "    p/(1-p) = exp(-3.28)\n",
    "    p = exp(-3.28)(1-p)\n",
    "    p = exp(-3.28) - exp(-3.28)*p\n",
    "    exp(-3.28) = p(1+exp(-3.28))\n",
    "    p = exp(-3.28) / (1 + exp(-3.28))\n",
    "\n",
    "    here it is\n",
    "    p = exp(log_odds) / (1 + exp(log_odds))\n",
    "\n",
    " \n",
    "In Python, we can do this simultaneously for all of the datapoints using NumPy (loaded as np):\n",
    "\n",
    "    np.exp(log_odds)/(1+ np.exp(log_odds))\n",
    "\n",
    "Output:\n",
    "\n",
    "array([[0.0361262 ],\n",
    "       [0.18775665],\n",
    "       [0.49490156],\n",
    "       [0.77262162],\n",
    "       [0.89887279]])\n",
    "\n",
    "The calculation that we just did required us to use something called the sigmoid function, which is the inverse of the logit function. The sigmoid function produces the S-shaped curve we saw previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##example\n",
    "# Import libraries and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "codecademyU = pd.read_csv('codecademyU.csv')\n",
    "\n",
    "# Fit the logistic regression model\n",
    "hours_studied = codecademyU[['hours_studied']]\n",
    "passed_exam = codecademyU[['passed_exam']]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(hours_studied,passed_exam)\n",
    "\n",
    "# Save intercept and coef\n",
    "intercept = model.intercept_\n",
    "coef = model.coef_\n",
    "\n",
    "# Calculate log_odds here\n",
    "log_odds = intercept + coef*hours_studied  #so we fit model then find log odds ln(p/1-p) now to get p from it\n",
    "print(log_odds)\n",
    "    #output is array of log odds\n",
    "# Calculate pred_probability_passing here\n",
    "pred_probability_passing = np.exp(log_odds)/(1+np.exp(log_odds)) #to find p we use this\n",
    "print(pred_probability_passing)\n",
    "    #output is array of p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec792ff5",
   "metadata": {},
   "source": [
    "## Fitting the model in Sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e352983",
   "metadata": {},
   "source": [
    "Fitting a model in sklearn\n",
    "Now that we’ve learned a little bit about how logistic regression works, let’s fit a model using sklearn.\n",
    "\n",
    "To do this, we’ll begin by importing the LogisticRegression module and creating a LogisticRegression object:\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "After creating the object, we need to fit our model on the data. We can accomplish this using the .fit() method, which takes two parameters: a matrix of features and a matrix of class labels (the outcome we are trying to predict).\n",
    "\n",
    "    model.fit(features, labels)\n",
    "\n",
    "Now that the model is trained, we can access a few useful attributes:\n",
    "\n",
    "    model.coef_ is a vector of the coefficients of each feature\n",
    "    model.intercept_ is the intercept\n",
    "The coefficients can be interpreted as follows:\n",
    "\n",
    "Large positive coefficient: a one unit increase in that feature is associated with a large increase in the log odds (and therefore probability) of a datapoint belonging to the positive class (the outcome group labeled as 1)\n",
    "Large negative coefficient: a one unit increase in that feature is associated with a large decrease in the log odds/probability of belonging to the positive class.\n",
    "Coefficient of 0: The feature is not associated with the outcome.\n",
    "\n",
    "## coeff u print both coeff and intercept and see\n",
    "\n",
    "    print(model.coef_)\n",
    "    print(model.intercept_)\n",
    "    coeff u see more positive and all we can say how that feature is responsible like more +ve means more important for prediction of good similary more negative means less important(or to other classify) so better print and see\n",
    "\n",
    "One important note is that sklearn‘s logistic regression implementation requires the features to be standardized because regularization is implemented by default.\n",
    "\n",
    "## so we shd do Strandardization \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X) for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cc_lr=LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Print the intercept and coefficients here:\n",
    "print(cc_lr.intercept_)\n",
    "print(cc_lr.coef_)\n",
    "#output is  \n",
    "#[-0.13173123] intercet\n",
    "#[[1.5100409  0.12002228]] coeff\n",
    "#the above ka interpretation is that\n",
    "#Both coefficients are positive, which makes sense: we expect students who study more and earn higher grades on the practice test to be more likely to pass the final exam. The coefficient on hours_studied is larger than the coefficient on practice_test, suggesting that hours_studied is more strongly associated with students’ probability of passing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5816c56",
   "metadata": {},
   "source": [
    "## Prediction in Sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a34e8df7",
   "metadata": {},
   "source": [
    "Predictions in sklearn\n",
    "Using a trained model, we can predict whether new datapoints belong to the positive class (the group labeled as 1) using the .predict() method. The input is a matrix of features and the output is a vector of predicted labels, 1 or 0.\n",
    "\n",
    "    print(model.predict(features))\n",
    "    # Sample output: [0 1 1 0 0]\n",
    "\n",
    "If we are more interested in the predicted probability of group membership, we can use the .predict_proba() method. The input to predict_proba() is also a matrix of features and the output is an array of probabilities, ranging from 0 to 1:\n",
    "\n",
    "    print(model.predict_proba(features)[:,1])\n",
    "    # Sample output: [0.32 0.75  0.55 0.20 0.44]\n",
    "\n",
    "By default, .predict_proba() returns the probability of class membership for both possible groups. In the example code above, we’ve only printed out the probability of belonging to the positive class. Notice that datapoints with predicted probabilities greater than 0.5 (the second and third datapoints in this example) were classified as 1s by the .predict() method. This is a process known as thresholding. As we can see here, sklearn sets the default classification threshold probability as 0.5.\n",
    "\n",
    "###so once we get probability p after odds and all we do we get p\n",
    "then seeing the threshold we classify wheather p is greater than 0.5 or not and we get 0 or 1\n",
    "the default threshold is set to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc_lr = LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Print out the predicted outcomes for the test data\n",
    "print(cc_lr.predict(X_test))\n",
    "#output is [0 1 0 1 1]\n",
    "\n",
    "# Print out the predicted probabilities for the test data\n",
    "print(cc_lr.predict_proba(X_test))\n",
    "print(cc_lr.predict_proba(X_test)[:,1])#only one ka means classified ka(to -ve ,to +ve prob) but gives only to +ve ka when [:,1] written\n",
    "# Print out the true outcomes for the test data\n",
    "print(y_test)\n",
    "#output is [0 1 0 0 1]\n",
    "#got wrong for 4th one as 4th ka prob we see very near near \n",
    "#You should see that the fourth datapoint was incorrectly classified as having passed the exam; however, the predicted probability of passing for this datapoint was only 57.7%, which is much lower than the other students who were correctly predicted to pass the exam (79.3% and 87.1%, respectively).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5609c",
   "metadata": {},
   "source": [
    "## Classification Threshold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6cd9a9c",
   "metadata": {},
   "source": [
    "Classification Thresholding\n",
    "As we’ve seen, logistic regression is used to predict the probability of group membership. Once we have this probability, we need to make a decision about what class a datapoint belongs to. This is where the classification threshold comes in!\n",
    "\n",
    "The default threshold for sklearn is 0.5. If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the datapoint is assigned to the positive class.\n",
    "\n",
    "Threshold at 0.5\n",
    "We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a logistic regression model that classifies whether or not an individual has cancer, we may want to be more sensitive to the positive cases. We wouldn’t want to tell someone they don’t have cancer when they actually do!\n",
    "\n",
    "In order to ensure that most patients with cancer are identified, we can move the classification threshold down to 0.3 or 0.4, increasing the sensitivity of our model to predicting a positive cancer classification. While this might result in more overall misclassifications, we are now missing fewer of the cases we are trying to detect: actual cancer patients.\n",
    "\n",
    "##Threshold is very important to think about"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7bc3b",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd99bc51",
   "metadata": {},
   "source": [
    "Confusion matrix\n",
    "When we fit a machine learning model, we need some way to evaluate it. Often, we do this by splitting our data into training and test datasets. We use the training data to fit the model; then we use the test set to see how well the model performs with new data.\n",
    "\n",
    "As a first step, data scientists often look at a confusion matrix, which shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "For example, suppose that the true and predicted classes for a logistic regression model are:\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "We can create a confusion matrix as follows:\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "Output:\n",
    "\n",
    "array([[3, 2],\n",
    "       [1, 4]])\n",
    "\n",
    "This output tells us that there are 3 true negatives, 1 false negative, 4 true positives, and 2 false positives. Ideally, we want the numbers on the main diagonal (in this case, 3 and 4, which are the true negatives and true positives, respectively) to be as large as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc_lr = LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Save and print the predicted outcomes\n",
    "y_pred = cc_lr.predict(X_test)\n",
    "print('predicted classes: ', y_pred)\n",
    "\n",
    "# Print out the true outcomes for the test data\n",
    "print('true classes: ', y_test)\n",
    "\n",
    "# Print out the confusion matrix here\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_pred,y_test)) #output is [[2 0] [1 2]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864659e",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10371d0e",
   "metadata": {},
   "source": [
    "Accuracy, Recall, Precision, F1 Score\n",
    "Once we have a confusion matrix, there are a few different statistics we can use to summarize the four values in the matrix. These include accuracy, precision, recall, and F1 score. We won’t go into much detail about these metrics here, but a quick summary is shown below (T = true, F = false, P = positive, N = negative). For all of these metrics, a value closer to 1 is better and closer to 0 is worse.\n",
    "\n",
    "Accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "Precision = TP/(TP + FP)\n",
    "Recall = TP/(TP + FN)\n",
    "F1 score: weighted average of precision and recall\n",
    "In sklearn, we can calculate these metrics as follows:\n",
    "\n",
    "# accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "# output: 0.7\n",
    "\n",
    "# precision:\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_true, y_pred))\n",
    "# output: 0.67\n",
    "\n",
    "# recall: \n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_true, y_pred))\n",
    "# output: 0.8\n",
    "\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred))\n",
    "# output: 0.73\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe058ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 51)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc_lr = LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Save and print the predicted outcomes\n",
    "y_pred = cc_lr.predict(X_test)\n",
    "print('predicted classes: ', y_pred)\n",
    "\n",
    "# Print out the true outcomes for the test data\n",
    "print('true classes: ', y_test)\n",
    "\n",
    "# Print out the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred)) # output is [[3 1] [0 1]]\n",
    "\n",
    "# Print accuracy here:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))# got 0.8\n",
    "\n",
    "# Print F1 score here:\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test,y_pred))#gor 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6105c4",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f4c4ddf",
   "metadata": {},
   "source": [
    "Review\n",
    "Congratulations! You just learned how a logistic regression model works and how to fit one to a dataset. Here are some of the things you learned:\n",
    "\n",
    "Logistic regression is used to perform binary classification.\n",
    "Logistic regression is an extension of linear regression where we use a logit link function to fit a sigmoid curve to the data, rather than a line.\n",
    "We can use the coefficients from a logistic regression model to estimate the log odds that a datapoint belongs to the positive class. We can then transform the log odds into a probability.\n",
    "The coefficients of a logistic regression model can be used to estimate relative feature importance.\n",
    "A classification threshold is used to determine the probabilistic cutoff for where a data sample is classified as belonging to a positive or negative class. The default cutoff in sklearn is 0.5.\n",
    "We can evaluate a logistic regression model using a confusion matrix or summary statistics such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab4f7874",
   "metadata": {},
   "source": [
    "!!!IMPORTANT!!!\n",
    "So it is nothing juat linear reg only a fit of line but cant classify so\n",
    "1.the model learns the b0 b1,b2 and all values b0->intercept b1,b2,...->weights and the values of x1,x2..->input values\n",
    "2.main thing is we need to standarsie before fitting\n",
    "3.so when we pass to predict for each input feature it calcultaes odds(p/(1-p)) for each of them then find probability for it with(exp(b0+b1*x1+b2*x2)/1+exp(b0+b1*x1+b2*x2))\n",
    "4.then classifies it based on probability \n",
    "5.the main threshold is 0.5(learn how to change)\n",
    "6. then we see accuracy, precision and recall,confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00562ad6",
   "metadata": {},
   "source": [
    "## Project-Log reg for breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe58a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae518edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Cancer_detection_KNN.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "132ef467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Unnamed: 32',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "779e1e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c8853c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "970c9c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "diagnosis                  0\n",
       "radius_mean                0\n",
       "texture_mean               0\n",
       "perimeter_mean             0\n",
       "area_mean                  0\n",
       "smoothness_mean            0\n",
       "compactness_mean           0\n",
       "concavity_mean             0\n",
       "concave points_mean        0\n",
       "symmetry_mean              0\n",
       "fractal_dimension_mean     0\n",
       "radius_se                  0\n",
       "texture_se                 0\n",
       "perimeter_se               0\n",
       "area_se                    0\n",
       "smoothness_se              0\n",
       "compactness_se             0\n",
       "concavity_se               0\n",
       "concave points_se          0\n",
       "symmetry_se                0\n",
       "fractal_dimension_se       0\n",
       "radius_worst               0\n",
       "texture_worst              0\n",
       "perimeter_worst            0\n",
       "area_worst                 0\n",
       "smoothness_worst           0\n",
       "compactness_worst          0\n",
       "concavity_worst            0\n",
       "concave points_worst       0\n",
       "symmetry_worst             0\n",
       "fractal_dimension_worst    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa975fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "count     569.000000  ...    569.000000     569.000000       569.000000   \n",
       "mean        0.181162  ...     16.269190      25.677223       107.261213   \n",
       "std         0.027414  ...      4.833242       6.146258        33.602542   \n",
       "min         0.106000  ...      7.930000      12.020000        50.410000   \n",
       "25%         0.161900  ...     13.010000      21.080000        84.110000   \n",
       "50%         0.179200  ...     14.970000      25.410000        97.660000   \n",
       "75%         0.195700  ...     18.790000      29.720000       125.400000   \n",
       "max         0.304000  ...     36.040000      49.540000       251.200000   \n",
       "\n",
       "        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "count            569.000000      569.000000               569.000000  \n",
       "mean               0.114606        0.290076                 0.083946  \n",
       "std                0.065732        0.061867                 0.018061  \n",
       "min                0.000000        0.156500                 0.055040  \n",
       "25%                0.064930        0.250400                 0.071460  \n",
       "50%                0.099930        0.282200                 0.080040  \n",
       "75%                0.161400        0.317900                 0.092080  \n",
       "max                0.291000        0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df9675af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2f3a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc71fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('diagnosis',axis=1)\n",
    "y=df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbdf897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "def scale(X):\n",
    "    for col in X.columns:\n",
    "        df[col]=scaler.fit_transform(df[col].values.reshape(-1,1))#remeber vvip\n",
    "    return df\n",
    "X=scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4c2531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.236405</td>\n",
       "      <td>M</td>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.236403</td>\n",
       "      <td>M</td>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.431741</td>\n",
       "      <td>M</td>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432121</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.432201</td>\n",
       "      <td>M</td>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-0.235732</td>\n",
       "      <td>M</td>\n",
       "      <td>2.110995</td>\n",
       "      <td>0.721473</td>\n",
       "      <td>2.060786</td>\n",
       "      <td>2.343856</td>\n",
       "      <td>1.041842</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>1.947285</td>\n",
       "      <td>2.320965</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901185</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>2.015301</td>\n",
       "      <td>0.378365</td>\n",
       "      <td>-0.273318</td>\n",
       "      <td>0.664512</td>\n",
       "      <td>1.629151</td>\n",
       "      <td>-1.360158</td>\n",
       "      <td>-0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>-0.235730</td>\n",
       "      <td>M</td>\n",
       "      <td>1.704854</td>\n",
       "      <td>2.085134</td>\n",
       "      <td>1.615931</td>\n",
       "      <td>1.723842</td>\n",
       "      <td>0.102458</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>1.263669</td>\n",
       "      <td>...</td>\n",
       "      <td>1.536720</td>\n",
       "      <td>2.047399</td>\n",
       "      <td>1.421940</td>\n",
       "      <td>1.494959</td>\n",
       "      <td>-0.691230</td>\n",
       "      <td>-0.394820</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.733827</td>\n",
       "      <td>-0.531855</td>\n",
       "      <td>-0.973978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-0.235727</td>\n",
       "      <td>M</td>\n",
       "      <td>0.702284</td>\n",
       "      <td>2.045574</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>-0.840484</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.105777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561361</td>\n",
       "      <td>1.374854</td>\n",
       "      <td>0.579001</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>-0.809587</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.326767</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>-1.104549</td>\n",
       "      <td>-0.318409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>-0.235725</td>\n",
       "      <td>M</td>\n",
       "      <td>1.838341</td>\n",
       "      <td>2.336457</td>\n",
       "      <td>1.982524</td>\n",
       "      <td>1.735218</td>\n",
       "      <td>1.525767</td>\n",
       "      <td>3.272144</td>\n",
       "      <td>3.296944</td>\n",
       "      <td>2.658866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961239</td>\n",
       "      <td>2.237926</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>1.653171</td>\n",
       "      <td>1.430427</td>\n",
       "      <td>3.904848</td>\n",
       "      <td>3.197605</td>\n",
       "      <td>2.289985</td>\n",
       "      <td>1.919083</td>\n",
       "      <td>2.219635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-0.242406</td>\n",
       "      <td>B</td>\n",
       "      <td>-1.808401</td>\n",
       "      <td>1.221792</td>\n",
       "      <td>-1.814389</td>\n",
       "      <td>-1.347789</td>\n",
       "      <td>-3.112085</td>\n",
       "      <td>-1.150752</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.410893</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.432735</td>\n",
       "      <td>-1.075813</td>\n",
       "      <td>-1.859019</td>\n",
       "      <td>-1.207552</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.048138</td>\n",
       "      <td>-0.751207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0   -0.236405         M     1.097064     -2.073335        1.269934   0.984375   \n",
       "1   -0.236403         M     1.829821     -0.353632        1.685955   1.908708   \n",
       "2    0.431741         M     1.579888      0.456187        1.566503   1.558884   \n",
       "3    0.432121         M    -0.768909      0.253732       -0.592687  -0.764464   \n",
       "4    0.432201         M     1.750297     -1.151816        1.776573   1.826229   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564 -0.235732         M     2.110995      0.721473        2.060786   2.343856   \n",
       "565 -0.235730         M     1.704854      2.085134        1.615931   1.723842   \n",
       "566 -0.235727         M     0.702284      2.045574        0.672676   0.577953   \n",
       "567 -0.235725         M     1.838341      2.336457        1.982524   1.735218   \n",
       "568 -0.242406         B    -1.808401      1.221792       -1.814389  -1.347789   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0           1.568466          3.283515        2.652874             2.532475   \n",
       "1          -0.826962         -0.487072       -0.023846             0.548144   \n",
       "2           0.942210          1.052926        1.363478             2.037231   \n",
       "3           3.283553          3.402909        1.915897             1.451707   \n",
       "4           0.280372          0.539340        1.371011             1.428493   \n",
       "..               ...               ...             ...                  ...   \n",
       "564         1.041842          0.219060        1.947285             2.320965   \n",
       "565         0.102458         -0.017833        0.693043             1.263669   \n",
       "566        -0.840484         -0.038680        0.046588             0.105777   \n",
       "567         1.525767          3.272144        3.296944             2.658866   \n",
       "568        -3.112085         -1.150752       -1.114873            -1.261820   \n",
       "\n",
       "     ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0    ...      1.886690      -1.359293         2.303601    2.001237   \n",
       "1    ...      1.805927      -0.369203         1.535126    1.890489   \n",
       "2    ...      1.511870      -0.023974         1.347475    1.456285   \n",
       "3    ...     -0.281464       0.133984        -0.249939   -0.550021   \n",
       "4    ...      1.298575      -1.466770         1.338539    1.220724   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...      1.901185       0.117700         1.752563    2.015301   \n",
       "565  ...      1.536720       2.047399         1.421940    1.494959   \n",
       "566  ...      0.561361       1.374854         0.579001    0.427906   \n",
       "567  ...      1.961239       2.237926         2.303601    1.653171   \n",
       "568  ...     -1.410893       0.764190        -1.432735   -1.075813   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0            1.307686           2.616665         2.109526   \n",
       "1           -0.375612          -0.430444        -0.146749   \n",
       "2            0.527407           1.082932         0.854974   \n",
       "3            3.394275           3.893397         1.989588   \n",
       "4            0.220556          -0.313395         0.613179   \n",
       "..                ...                ...              ...   \n",
       "564          0.378365          -0.273318         0.664512   \n",
       "565         -0.691230          -0.394820         0.236573   \n",
       "566         -0.809587           0.350735         0.326767   \n",
       "567          1.430427           3.904848         3.197605   \n",
       "568         -1.859019          -1.207552        -1.305831   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                2.296076        2.750622                 1.937015  \n",
       "1                1.087084       -0.243890                 0.281190  \n",
       "2                1.955000        1.152255                 0.201391  \n",
       "3                2.175786        6.046041                 4.935010  \n",
       "4                0.729259       -0.868353                -0.397100  \n",
       "..                    ...             ...                      ...  \n",
       "564              1.629151       -1.360158                -0.709091  \n",
       "565              0.733827       -0.531855                -0.973978  \n",
       "566              0.414069       -1.104549                -0.318409  \n",
       "567              2.289985        1.919083                 2.219635  \n",
       "568             -1.745063       -0.048138                -0.751207  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5534167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop('diagnosis',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc9b756f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.236405</td>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>2.217515</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.236403</td>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.431741</td>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432121</td>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>2.867383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.432201</td>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-0.235732</td>\n",
       "      <td>2.110995</td>\n",
       "      <td>0.721473</td>\n",
       "      <td>2.060786</td>\n",
       "      <td>2.343856</td>\n",
       "      <td>1.041842</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>1.947285</td>\n",
       "      <td>2.320965</td>\n",
       "      <td>-0.312589</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901185</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>2.015301</td>\n",
       "      <td>0.378365</td>\n",
       "      <td>-0.273318</td>\n",
       "      <td>0.664512</td>\n",
       "      <td>1.629151</td>\n",
       "      <td>-1.360158</td>\n",
       "      <td>-0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>-0.235730</td>\n",
       "      <td>1.704854</td>\n",
       "      <td>2.085134</td>\n",
       "      <td>1.615931</td>\n",
       "      <td>1.723842</td>\n",
       "      <td>0.102458</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>1.263669</td>\n",
       "      <td>-0.217664</td>\n",
       "      <td>...</td>\n",
       "      <td>1.536720</td>\n",
       "      <td>2.047399</td>\n",
       "      <td>1.421940</td>\n",
       "      <td>1.494959</td>\n",
       "      <td>-0.691230</td>\n",
       "      <td>-0.394820</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.733827</td>\n",
       "      <td>-0.531855</td>\n",
       "      <td>-0.973978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-0.235727</td>\n",
       "      <td>0.702284</td>\n",
       "      <td>2.045574</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>-0.840484</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.105777</td>\n",
       "      <td>-0.809117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561361</td>\n",
       "      <td>1.374854</td>\n",
       "      <td>0.579001</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>-0.809587</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.326767</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>-1.104549</td>\n",
       "      <td>-0.318409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>-0.235725</td>\n",
       "      <td>1.838341</td>\n",
       "      <td>2.336457</td>\n",
       "      <td>1.982524</td>\n",
       "      <td>1.735218</td>\n",
       "      <td>1.525767</td>\n",
       "      <td>3.272144</td>\n",
       "      <td>3.296944</td>\n",
       "      <td>2.658866</td>\n",
       "      <td>2.137194</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961239</td>\n",
       "      <td>2.237926</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>1.653171</td>\n",
       "      <td>1.430427</td>\n",
       "      <td>3.904848</td>\n",
       "      <td>3.197605</td>\n",
       "      <td>2.289985</td>\n",
       "      <td>1.919083</td>\n",
       "      <td>2.219635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-0.242406</td>\n",
       "      <td>-1.808401</td>\n",
       "      <td>1.221792</td>\n",
       "      <td>-1.814389</td>\n",
       "      <td>-1.347789</td>\n",
       "      <td>-3.112085</td>\n",
       "      <td>-1.150752</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.820070</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.410893</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.432735</td>\n",
       "      <td>-1.075813</td>\n",
       "      <td>-1.859019</td>\n",
       "      <td>-1.207552</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.048138</td>\n",
       "      <td>-0.751207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0   -0.236405     1.097064     -2.073335        1.269934   0.984375   \n",
       "1   -0.236403     1.829821     -0.353632        1.685955   1.908708   \n",
       "2    0.431741     1.579888      0.456187        1.566503   1.558884   \n",
       "3    0.432121    -0.768909      0.253732       -0.592687  -0.764464   \n",
       "4    0.432201     1.750297     -1.151816        1.776573   1.826229   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564 -0.235732     2.110995      0.721473        2.060786   2.343856   \n",
       "565 -0.235730     1.704854      2.085134        1.615931   1.723842   \n",
       "566 -0.235727     0.702284      2.045574        0.672676   0.577953   \n",
       "567 -0.235725     1.838341      2.336457        1.982524   1.735218   \n",
       "568 -0.242406    -1.808401      1.221792       -1.814389  -1.347789   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0           1.568466          3.283515        2.652874             2.532475   \n",
       "1          -0.826962         -0.487072       -0.023846             0.548144   \n",
       "2           0.942210          1.052926        1.363478             2.037231   \n",
       "3           3.283553          3.402909        1.915897             1.451707   \n",
       "4           0.280372          0.539340        1.371011             1.428493   \n",
       "..               ...               ...             ...                  ...   \n",
       "564         1.041842          0.219060        1.947285             2.320965   \n",
       "565         0.102458         -0.017833        0.693043             1.263669   \n",
       "566        -0.840484         -0.038680        0.046588             0.105777   \n",
       "567         1.525767          3.272144        3.296944             2.658866   \n",
       "568        -3.112085         -1.150752       -1.114873            -1.261820   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         2.217515  ...      1.886690      -1.359293         2.303601   \n",
       "1         0.001392  ...      1.805927      -0.369203         1.535126   \n",
       "2         0.939685  ...      1.511870      -0.023974         1.347475   \n",
       "3         2.867383  ...     -0.281464       0.133984        -0.249939   \n",
       "4        -0.009560  ...      1.298575      -1.466770         1.338539   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564      -0.312589  ...      1.901185       0.117700         1.752563   \n",
       "565      -0.217664  ...      1.536720       2.047399         1.421940   \n",
       "566      -0.809117  ...      0.561361       1.374854         0.579001   \n",
       "567       2.137194  ...      1.961239       2.237926         2.303601   \n",
       "568      -0.820070  ...     -1.410893       0.764190        -1.432735   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2.001237          1.307686           2.616665         2.109526   \n",
       "1      1.890489         -0.375612          -0.430444        -0.146749   \n",
       "2      1.456285          0.527407           1.082932         0.854974   \n",
       "3     -0.550021          3.394275           3.893397         1.989588   \n",
       "4      1.220724          0.220556          -0.313395         0.613179   \n",
       "..          ...               ...                ...              ...   \n",
       "564    2.015301          0.378365          -0.273318         0.664512   \n",
       "565    1.494959         -0.691230          -0.394820         0.236573   \n",
       "566    0.427906         -0.809587           0.350735         0.326767   \n",
       "567    1.653171          1.430427           3.904848         3.197605   \n",
       "568   -1.075813         -1.859019          -1.207552        -1.305831   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                2.296076        2.750622                 1.937015  \n",
       "1                1.087084       -0.243890                 0.281190  \n",
       "2                1.955000        1.152255                 0.201391  \n",
       "3                2.175786        6.046041                 4.935010  \n",
       "4                0.729259       -0.868353                -0.397100  \n",
       "..                    ...             ...                      ...  \n",
       "564              1.629151       -1.360158                -0.709091  \n",
       "565              0.733827       -0.531855                -0.973978  \n",
       "566              0.414069       -1.104549                -0.318409  \n",
       "567              2.289985        1.919083                 2.219635  \n",
       "568             -1.745063       -0.048138                -0.751207  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d938a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "888a1981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log=LogisticRegression()\n",
    "log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dee0c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e05b5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea6024a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70  0]\n",
      " [ 0 44]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b8b12ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAGiCAYAAADp4c+XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAilUlEQVR4nO3dcXBU5b3/8c9WYEkwRAiwm0XQVBevClgFG4liQmtiqcVSWh0LWhyu/sIEW2P0h41xLrFjd2vuGONtKiO0Yqil9PZWEeygibfXoI20IZRW05bqJT9AZcnNNZKAcRPY8/vD6dp9EkgWNtn1nPerc2bMc07O+Wamztfv93nOeVyWZVkCAACO8ZlkBwAAAEYWyR8AAIch+QMA4DAkfwAAHIbkDwCAw5D8AQBwGJI/AAAOQ/IHAMBhSP4AADgMyR8AAIch+QMAkCLOP/98uVyufseqVaskSZZlqbKyUj6fT2lpaSooKFBra2vczyH5AwCQIpqbm3Xo0KHo0dDQIEm66aabJElVVVWqrq5WbW2tmpub5fV6VVhYqO7u7rie42JjHwAAUlNpaaleeOEFvfXWW5Ikn8+n0tJS3X///ZKkcDgsj8ejRx55RMXFxUO+L5U/AADDKBwOq6urK+YIh8OD/l5vb6+eeeYZrVixQi6XS21tbQqFQioqKope43a7lZ+fr6amprhiGhX3XzFM+jr2JTsEIOWk+eYnOwQgJR3vfXdY75/InBSs3aiHHnooZmzNmjWqrKw85e9t2bJFH3zwgW6//XZJUigUkiR5PJ6Y6zwej/bv3x9XTCmT/AEASBmREwm7VXl5ucrKymLG3G73oL/3k5/8RAsXLpTP54sZd7lcMT9bltVvbDAkfwAAhpHb7R5Ssv9H+/fv18svv6xnn302Oub1eiV93AHIzs6Ojre3t/frBgyGOX8AAExWJHHHadiwYYOmTJmiG264ITqWk5Mjr9cbfQNA+nhdQGNjo/Ly8uK6P5U/AACmyOkl7cQ8OqINGzZo+fLlGjXqkzTtcrlUWlqqQCAgv98vv9+vQCCg9PR0LV26NK5nkPwBADBYp1mxJ8LLL7+sAwcOaMWKFf3OrV69Wj09PSopKVFnZ6dyc3NVX1+vjIyMuJ6RMu/5s9of6I/V/sDAhnu1f+978X8172TG+C5N2L0ShcofAABTEtv+I4HkDwCAKYlt/5HAan8AAByGyh8AAFMCP/KTikj+AACYaPsDAAA7ofIHAMDEan8AAJwlmR/5GQm0/QEAcBgqfwAATLT9AQBwGJu3/Un+AACYbP6eP3P+AAA4DJU/AAAm2v4AADiMzRf80fYHAMBhqPwBADDR9gcAwGFo+wMAADuh8gcAwGBZ9n7Pn+QPAIDJ5nP+tP0BAHAYKn8AAEw2X/BH8gcAwGTztj/JHwAAExv7AAAAO6HyBwDARNsfAACHsfmCP9r+AAA4DJU/AAAm2v4AADgMbX8AAGAnVP4AAJhsXvmT/AEAMNh9Vz/a/gAAOAyVPwAAJtr+AAA4DK/6AQDgMDav/JnzBwDAYaj8AQAw0fYHAMBhaPsDAAA7IfkDAGCyIok74vTuu+/q1ltvVVZWltLT0/W5z31OLS0tn4RmWaqsrJTP51NaWpoKCgrU2toa1zNI/gAAmCKRxB1x6Ozs1NVXX63Ro0dr+/bt+vOf/6xHH31U55xzTvSaqqoqVVdXq7a2Vs3NzfJ6vSosLFR3d/eQn8OcPwAAwygcDiscDseMud1uud3uftc+8sgjmjZtmjZs2BAdO//886P/bFmWampqVFFRoSVLlkiS6urq5PF4tGnTJhUXFw8pJip/AABMCaz8g8GgMjMzY45gMDjgY7du3aq5c+fqpptu0pQpU3T55Zdr/fr10fNtbW0KhUIqKiqKjrndbuXn56upqWnIfx7JHwAAUwLn/MvLy3XkyJGYo7y8fMDH7tu3T2vXrpXf79dLL72klStX6jvf+Y42btwoSQqFQpIkj8cT83sejyd6biho+wMAMIxO1uIfSCQS0dy5cxUIBCRJl19+uVpbW7V27Vp961vfil7ncrlifs+yrH5jp0LlDwCAKUkL/rKzs3XJJZfEjF188cU6cOCAJMnr9UpSvyq/vb29XzfgVEj+AACYkvSq39VXX629e/fGjP3tb3/TeeedJ0nKycmR1+tVQ0ND9Hxvb68aGxuVl5c35OfQ9gcAwJSkL/zdc889ysvLUyAQ0M0336zf//73WrdundatWyfp43Z/aWmpAoGA/H6//H6/AoGA0tPTtXTp0iE/h+QPAECKuPLKK/Xcc8+pvLxc3/ve95STk6OamhotW7Yses3q1avV09OjkpISdXZ2Kjc3V/X19crIyBjyc1yWZVnD8QfEq69jX7JDAFJOmm9+skMAUtLx3neH9f49zwYSdq+0JQ8k7F6JQuUPAICJjX0AAICdUPkDAGCyeeVP8gcAwJQay+GGDW1/AAAchsofAAATbX8AABzG5smftj8AAA5D5Q8AgCnOb/J/2pD8AQAw2bztT/IHAMDEq34AAMBOqPwBADDR9gcAwGFsnvxp+wMA4DBU/gAAmHjVDwAAZ7EirPYHAAA2QuUPAIDJ5gv+SP4AAJhsPudP2x8AAIeh8gcAwGTzBX8kfwAATMz5AwDgMDZP/sz5AwDgMFT+AACYbL6lL8kfAACTzdv+JH8HKvr6cr0Xau83fsuSr+jBe1fJsiw98dTP9B/Pb1dX91HNuvQiPVi2Shd+9rwkRAsk38ri5bq3bKWys6eo9c9/0733rtFrv/19ssMCThvJ34E2//hxRf7hv2rf2rdfd5Y+oKIF8yVJT/3sl9q4+Vk9XHGvzp8+VU8+/XPdWfqAXvj5eo0bl56ssIGkuOmmG1X9aKXu+vYDanq9WXfecZte2PaMZl1WoIMH30t2eBguNn/VjwV/DjRxwjmalDUxejT+9neaNjVbV14+S5Zl6af/vkX/Z/ktKiy4Wv7Pnq/Ag/fqo3BYv254JdmhAyPunrvv1FMbNuupDT/XX//6tu69b40OvvOeVhZ/K9mhYThZkcQdKSju5P/OO++ooqJCCxYs0MUXX6xLLrlECxYsUEVFhQ4ePDgcMWIY9fX16YX6/9LXbiiSy+XSO++F1PG/ncr7/BXRa8aMGaO5n5ulPW/8OYmRAiNv9OjRuuKK2Wp4uTFmvKGhUfOumpukqIAzF1fb/7XXXtPChQs1bdo0FRUVqaioSJZlqb29XVu2bNEPf/hDbd++XVdfffUp7xMOhxUOh2PGPhMOy+12x/8X4Iz8547X1X30qBZ/uVCS1PF+pyQpa8KEmOuyJp4z4DoBwM4mTZqoUaNGqf1wR8x4e3uHPN4pSYoKI8Lmbf+4kv8999yjO+64Q4899thJz5eWlqq5ufmU9wkGg3rooYdixh78v9/Rv6y+O55wkADPvvCSrrlqrqZMzooZd7lcMT9bVv8xwCks47Uvl8vVbwz2Ytl8tX9cbf8333xTK1euPOn54uJivfnmm4Pep7y8XEeOHIk57r/75PfF8HgvdFg7d+3R1xd9KTo2aeLHFX/H++/HXPt+5wfKmnDOSIYHJF1Hx/s6fvy4PN7JMeOTJ2ep/fD/JCkq4MzFlfyzs7PV1NR00vOvv/66srOzB72P2+3W+PHjYw5a/iPvuV83aOKETF077/PRsXN9Xk3KmqDXm/8QHevr69OuPW/oc7MuSUaYQNL09fVp9+4/6bovXhszft111+r1nbuSFBVGRMRK3JGC4mr733fffVq5cqVaWlpUWFgoj8cjl8ulUCikhoYG/fjHP1ZNTc0whYpEikQi2vLrBn114XUaNeqs6LjL5dJtNy/W+o2/0PRzfTpv2lSt3/gLjXW7dUNhQfICBpLkscfXq27D42pp+aN2/q5Fd/7zrZo+baqeXPfTZIeG4ZSiq/QTJa7kX1JSoqysLD322GN68skndeLECUnSWWedpTlz5mjjxo26+eabhyVQJNbrzX/QocPt+toNRf3OrVh2kz4K9+rhR3+kru6jmn3JRVpX833e8Ycj/fKXW5U1cYIerLhH2dlT9GbrXi268TYdOPBuskPDcErRij1RXNZprlrp6+tTR8fHK2AnTZqk0aNHn1EgfR37zuj3ATtK881PdghASjreO7z/8XXse8sSdq9x//KzhN0rUU77C3+jR48e0vw+AACfOjZf7c/nfQEAMNm87c/nfQEAcBgqfwAATDZf7U/lDwCAKUnv+VdWVsrlcsUcXq83et6yLFVWVsrn8yktLU0FBQVqbW2N+88j+QMAkEIuvfRSHTp0KHq88cYb0XNVVVWqrq5WbW2tmpub5fV6VVhYqO7u7rieQdsfAABDIr/tP9Bmdm63+6Rfth01alRMtR+NybJUU1OjiooKLVmyRJJUV1cnj8ejTZs2qbi4eMgxUfkDAGBKYNs/GAwqMzMz5ggGgyd99FtvvSWfz6ecnBzdcsst2rfv4+/gtLW1KRQKqajok4+zud1u5efnn/LT+wOh8gcAYBiVl5errKwsZuxkVX9ubq42btyoGTNm6PDhw3r44YeVl5en1tZWhUIhSZLH44n5HY/Ho/3798cVE8kfAABTAt/zP1WL37Rw4cLoP8+aNUvz5s3TBRdcoLq6Ol111VWSBtpy3Yp7y3Xa/gAAmKxI4o4zMG7cOM2aNUtvvfVWdB3A3zsAf9fe3t6vGzAYkj8AAKYU2dI3HA7rL3/5i7Kzs5WTkyOv16uGhobo+d7eXjU2NiovLy+u+9L2BwAgRdx3331atGiRpk+frvb2dj388MPq6urS8uXL5XK5VFpaqkAgIL/fL7/fr0AgoPT0dC1dujSu55D8AQAwWEn6tv8777yjb37zm+ro6NDkyZN11VVXaefOnTrvvPMkSatXr1ZPT49KSkrU2dmp3Nxc1dfXKyMjI67nnPaWvonGlr5Af2zpCwxsuLf07f7OVxJ2r4x/eyFh90oU5vwBAHAY2v4AAJgS+IW/VETyBwDAlKQ5/5FC2x8AAIeh8gcAwGTzyp/kDwCAIUVehBs2tP0BAHAYKn8AAEy0/QEAcBiSPwAAzpKsz/uOFOb8AQBwGCp/AABMNq/8Sf4AAJjs/XVf2v4AADgNlT8AAAa7L/gj+QMAYLJ58qftDwCAw1D5AwBgsvmCP5I/AAAGu8/50/YHAMBhqPwBADDR9gcAwFns3vYn+QMAYLJ55c+cPwAADkPlDwCAwbJ55U/yBwDAZPPkT9sfAACHofIHAMBA2x8AAKexefKn7Q8AgMNQ+QMAYKDtDwCAw5D8AQBwGLsnf+b8AQBwGCp/AABMlivZEQwrkj8AAAba/gAAwFao/AEAMFgR2v4AADgKbX8AAGArVP4AABgsm6/2p/IHAMBgRRJ3nK5gMCiXy6XS0tJP4rIsVVZWyufzKS0tTQUFBWptbY373iR/AABSTHNzs9atW6fZs2fHjFdVVam6ulq1tbVqbm6W1+tVYWGhuru747o/yR8AAIMVcSXsiNfRo0e1bNkyrV+/XhMmTPgkJstSTU2NKioqtGTJEs2cOVN1dXX68MMPtWnTprieQfIHAMBgWYk7wuGwurq6Yo5wOHzSZ69atUo33HCDrrvuupjxtrY2hUIhFRUVRcfcbrfy8/PV1NQU199H8gcAwJDIyj8YDCozMzPmCAaDAz538+bN2r1794DnQ6GQJMnj8cSMezye6LmhYrU/AADDqLy8XGVlZTFjbre733UHDx7U3Xffrfr6eo0dO/ak93O5YqcSLMvqNzYYkj8AAIZEfuHP7XYPmOxNLS0tam9v15w5c6JjJ06c0I4dO1RbW6u9e/dK+rgDkJ2dHb2mvb29XzdgMLT9AQAwJHLOf6i++MUv6o033tCePXuix9y5c7Vs2TLt2bNHn/3sZ+X1etXQ0BD9nd7eXjU2NiovLy+uv4/KHwCAFJCRkaGZM2fGjI0bN05ZWVnR8dLSUgUCAfn9fvn9fgUCAaWnp2vp0qVxPYvkDwCAIVU39lm9erV6enpUUlKizs5O5ebmqr6+XhkZGXHdx2VZ8TQlhk9fx75khwCknDTf/GSHAKSk473vDuv9/3vm9Qm71wVvvpSweyUKc/4AADgMbX8AAAx239KX5A8AgCHCrn4AAMBOqPwBADBYNq/8Sf4AABhS9VW/RCH5AwBgSI2X4IcPc/4AADgMlT8AAAba/gAAOAyv+gEAAFuh8gcAwMCrfgAAOAyr/QEAgK1Q+QMAYLD7gj+SPwAABrvP+dP2BwDAYaj8AQAw2H3BH8kfAAADc/4jJM03P9khACnn8PUXJjsEwJGY8wcAALaSMpU/AACpgrY/AAAOY/P1frT9AQBwGip/AAAMtP0BAHAYVvsDAABbofIHAMAQSXYAw4zkDwCAwRJtfwAAYCNU/gAAGCI2f9Gf5A8AgCFi87Y/yR8AAANz/gAAwFao/AEAMPCqHwAADkPbHwAA2AqVPwAABtr+AAA4jN2TP21/AAAchsofAACD3Rf8kfwBADBE7J37afsDAJAq1q5dq9mzZ2v8+PEaP3685s2bp+3bt0fPW5alyspK+Xw+paWlqaCgQK2trXE/h+QPAIAhIlfCjnice+65+sEPfqBdu3Zp165d+sIXvqCvfvWr0QRfVVWl6upq1dbWqrm5WV6vV4WFheru7o7rOSR/AAAMVgKPeCxatEhf/vKXNWPGDM2YMUPf//73dfbZZ2vnzp2yLEs1NTWqqKjQkiVLNHPmTNXV1enDDz/Upk2b4noOyR8AAEMkgUc4HFZXV1fMEQ6HB43hxIkT2rx5s44dO6Z58+apra1NoVBIRUVF0Wvcbrfy8/PV1NQU199H8gcAYBgFg0FlZmbGHMFg8KTXv/HGGzr77LPldru1cuVKPffcc7rkkksUCoUkSR6PJ+Z6j8cTPTdUrPYHAMAQcSVuuX95ebnKyspixtxu90mvv+iii7Rnzx598MEH+tWvfqXly5ersbExet5lxGZZVr+xwZD8AQAwxDtXfyput/uUyd40ZswYXXjhhZKkuXPnqrm5WY8//rjuv/9+SVIoFFJ2dnb0+vb29n7dgMHQ9gcAIIVZlqVwOKycnBx5vV41NDREz/X29qqxsVF5eXlx3ZPKHwAAQ7K+7f/AAw9o4cKFmjZtmrq7u7V582a98sorevHFF+VyuVRaWqpAICC/3y+/369AIKD09HQtXbo0rueQ/AEAMCTrC3+HDx/WbbfdpkOHDikzM1OzZ8/Wiy++qMLCQknS6tWr1dPTo5KSEnV2dio3N1f19fXKyMiI6zkuy7ISObVx2kaNmZrsEICUc/j6C5MdApCSsrY1Dn7RGfi5b1nC7vXN936WsHslCpU/AACGeL/M92lD8gcAwJASLfFhxGp/AAAchsofAACD3bf0JfkDAGBI1qt+I4XkDwCAgTl/AABgK1T+AAAYmPMHAMBh7D7nT9sfAACHofIHAMBg98qf5A8AgMGy+Zw/bX8AAByGyh8AAANtfwAAHMbuyZ+2PwAADkPlDwCAwe6f9yX5AwBg4At/AAA4DHP+AADAVqj8AQAw2L3yJ/kDAGCw+4I/2v4AADgMlT8AAAZW+wMA4DB2n/On7Q8AgMNQ+QMAYLD7gj+SPwAAhojN0z9tfwAAHIbKHwAAg90X/JH8AQAw2LvpT/IHAKAfu1f+zPkDAOAwVP4AABj4wh8AAA7Dq34AAMBWqPwBADDYu+4n+QMA0A+r/QEAgK1Q+QMAYLD7gj+SPwAABnunftr+AAA4DskfAABDJIFHPILBoK688kplZGRoypQpWrx4sfbu3RtzjWVZqqyslM/nU1pamgoKCtTa2hrXc0j+AAAYIrISdsSjsbFRq1at0s6dO9XQ0KDjx4+rqKhIx44di15TVVWl6upq1dbWqrm5WV6vV4WFheru7h7yc5jzBwDAkKw5/xdffDHm5w0bNmjKlClqaWnRtddeK8uyVFNTo4qKCi1ZskSSVFdXJ4/Ho02bNqm4uHhIz6HyBwBgGIXDYXV1dcUc4XB4SL975MgRSdLEiRMlSW1tbQqFQioqKope43a7lZ+fr6ampiHHRPIHAMCQyDn/YDCozMzMmCMYDA4ag2VZKisr0zXXXKOZM2dKkkKhkCTJ4/HEXOvxeKLnhoK2PwAABiuBjf/y8nKVlZXFjLnd7kF/76677tKf/vQnvfbaa/3OuVyx2w5altVv7FRI/gAADCO32z2kZP+Pvv3tb2vr1q3asWOHzj333Oi41+uV9HEHIDs7Ozre3t7erxtwKrT9AQAwJOtVP8uydNddd+nZZ5/Vb37zG+Xk5MScz8nJkdfrVUNDQ3Sst7dXjY2NysvLG/JzqPwBADAk6/O+q1at0qZNm/T8888rIyMjOo+fmZmptLQ0uVwulZaWKhAIyO/3y+/3KxAIKD09XUuXLh3yc0j+AACkiLVr10qSCgoKYsY3bNig22+/XZK0evVq9fT0qKSkRJ2dncrNzVV9fb0yMjKG/BySPwAAhmS9529Zgz/Z5XKpsrJSlZWVp/0ckj8AAAa77+rHgj9ErSxerrf2vq6jXf+t3+3crmuu/nyyQwKSZuw3lilrW6PS77hrwPPjVt2rrG2NGnvjN0Y4MuDMkfwhSbrpphtV/Wilgj/4N839/PV67bXf64Vtz2jaNF+yQwNG3Fn+f9LYLy3S8ba3Bzw/+qprNGrGxYr87/+McGQYKcla7T9SSP6QJN1z9516asNmPbXh5/rrX9/Wvfet0cF33tPK4m8lOzRgZI1NU8a9D+rYD/9V1tH+G6V8ZuIkjSu+W0cffVjW8eNJCBAjwUrg/1IRyR8aPXq0rrhithpebowZb2ho1Lyr5iYpKiA5xq0sVe+u19X3x5b+J10unV1WoY+e3awTB/7fiMeGkUPlH6eDBw9qxYoVp7xmoE0OhrLCEcNj0qSJGjVqlNoPd8SMt7d3yOOdkqSogJE3Zv4XNOqCGfqwbv2A58d+famsyAl9tO1XIxwZkFgJT/7vv/++6urqTnnNQJscWJGh70OM4WH+B5jL5eI/yuAYn5k0WePu/LaOPvqw1Nfb7/xZF8xQ2o1f19GawTdkwaef3dv+cb/qt3Xr1lOe37dv36D3GGiTgwlZ/xRvKEiQjo73dfz4cXm8k2PGJ0/OUvthFjTBGc668CJ9ZsJEZdasi465zhqlUZdeprFf+Zo+fPpJuTInaMJT/x5zPn1Ficbe+A19cMctyQgbwyRV2/WJEnfyX7x48aAV4WA7Cw20yUE8uxEhsfr6+rR795903Rev1fPPvxgdv+66a7Vt20tJjAwYOX1/bNEHq26PGTu79Ls68c4B9fzHJkU6/1d9u5tjzo//3r8q/F/1+ujl7SMYKXDm4k7+2dnZ+tGPfqTFixcPeH7Pnj2aM2fOmcaFEfbY4+tVt+FxtbT8UTt/16I7//lWTZ82VU+u+2myQwNGRk+PThxoixmyPupRpOtIdPxEd1fs+ePHFel8X5F3D45YmBgZEZtPecad/OfMmaPdu3efNPkzT/zp9MtfblXWxAl6sOIeZWdP0Zute7Xoxtt04MC7yQ4NAEac3bOYy4ozU7/66qs6duyYvvSlLw14/tixY9q1a5fy8/PjCmTUmKlxXQ84weHrL0x2CEBKytrWOPhFZ+DW85Yk7F7P7H82YfdKlLgr//nz55/y/Lhx4+JO/AAApBK7f9ufjX0AADCk6it6icIX/gAAcBgqfwAADLznDwCAwzDnDwCAwzDnDwAAbIXKHwAAA3P+AAA4jN2/VEvbHwAAh6HyBwDAwGp/AAAcxu5z/rT9AQBwGCp/AAAMdn/Pn+QPAIDB7nP+tP0BAHAYKn8AAAx2f8+f5A8AgMHuq/1J/gAAGOy+4I85fwAAHIbKHwAAg91X+5P8AQAw2H3BH21/AAAchsofAAADbX8AAByG1f4AAMBWqPwBADBEbL7gj+QPAIDB3qmftj8AAI5D5Q8AgMHuq/2p/AEAMERkJeyIx44dO7Ro0SL5fD65XC5t2bIl5rxlWaqsrJTP51NaWpoKCgrU2toa999H8gcAwGBZVsKOeBw7dkyXXXaZamtrBzxfVVWl6upq1dbWqrm5WV6vV4WFheru7o7rObT9AQAYRuFwWOFwOGbM7XbL7Xb3u3bhwoVauHDhgPexLEs1NTWqqKjQkiVLJEl1dXXyeDzatGmTiouLhxwTlT8AAIZEtv2DwaAyMzNjjmAwGHdMbW1tCoVCKioqio653W7l5+erqakprntR+QMAYEjkF/7Ky8tVVlYWMzZQ1T+YUCgkSfJ4PDHjHo9H+/fvj+teJH8AAIbRyVr8p8vlcsX8bFlWv7HB0PYHAMCQrAV/p+L1eiV90gH4u/b29n7dgMGQ/AEAMCTrVb9TycnJkdfrVUNDQ3Sst7dXjY2NysvLi+tetP0BAEgRR48e1dtvvx39ua2tTXv27NHEiRM1ffp0lZaWKhAIyO/3y+/3KxAIKD09XUuXLo3rOSR/AAAMiWzXx2PXrl1asGBB9Oe/LxRcvny5nn76aa1evVo9PT0qKSlRZ2encnNzVV9fr4yMjLie47KS9RcaRo2ZmuwQgJRz+PoLkx0CkJKytjUO6/0v88bXRj+VP4biew1vJDDnDwCAw9D2BwDAkMj3/FMRyR8AAEMkNWbEhw3JHwAAg90rf+b8AQBwGCp/AAAMtP0BAHAY2v4AAMBWqPwBADDQ9gcAwGFo+wMAAFuh8gcAwEDbHwAAh6HtDwAAbIXKHwAAg2VFkh3CsCL5AwBgiNi87U/yBwDAYNl8wR9z/gAAOAyVPwAABtr+AAA4DG1/AABgK1T+AAAY+MIfAAAOwxf+AACArVD5AwBgsPuCP5I/AAAGu7/qR9sfAACHofIHAMBA2x8AAIfhVT8AABzG7pU/c/4AADgMlT8AAAa7r/Yn+QMAYKDtDwAAbIXKHwAAA6v9AQBwGDb2AQAAtkLlDwCAgbY/AAAOw2p/AABgK1T+AAAY7L7gj+QPAICBtj8AAA5jWVbCjng98cQTysnJ0dixYzVnzhy9+uqrCf/7SP4AAKSIX/ziFyotLVVFRYX+8Ic/aP78+Vq4cKEOHDiQ0Oe4rBTpbYwaMzXZIQAp5/D1FyY7BCAlZW1rHNb7JzInHevep3A4HDPmdrvldrv7XZubm6srrrhCa9eujY5dfPHFWrx4sYLBYMJiSpk5/+O97yY7BEgKh8MKBoMqLy8f8P+YgBPx74XzJDInVVZW6qGHHooZW7NmjSorK2PGent71dLSou9+97sx40VFRWpqakpYPFIKVf5IDV1dXcrMzNSRI0c0fvz4ZIcDpAT+vcCZCIfDQ6r833vvPU2dOlW//e1vlZeXFx0PBAKqq6vT3r17ExZTylT+AADY0cla/CfjcrlifrYsq9/YmWLBHwAAKWDSpEk666yzFAqFYsbb29vl8XgS+iySPwAAKWDMmDGaM2eOGhoaYsYbGhpipgESgbY/Yrjdbq1Zs4ZFTcA/4N8LjJSysjLddtttmjt3rubNm6d169bpwIEDWrlyZUKfw4I/AABSyBNPPKGqqiodOnRIM2fO1GOPPaZrr702oc8g+QMA4DDM+QMA4DAkfwAAHIbkDwCAw5D8AQBwGJI/okZiG0ng02THjh1atGiRfD6fXC6XtmzZkuyQgIQg+UPSyG0jCXyaHDt2TJdddplqa2uTHQqQULzqB0kjt40k8Gnlcrn03HPPafHixckOBThjVP6IbiNZVFQUMz4c20gCAJKP5A91dHToxIkT/TaO8Hg8/TaYAAB8+pH8ETUS20gCAJKP5I8R3UYSAJB8JH+M6DaSAIDkY0tfSBq5bSSBT5OjR4/q7bffjv7c1tamPXv2aOLEiZo+fXoSIwPODK/6IWoktpEEPk1eeeUVLViwoN/48uXL9fTTT498QECCkPwBAHAY5vwBAHAYkj8AAA5D8gcAwGFI/gAAOAzJHwAAhyH5AwDgMCR/AAAchuQPAIDDkPwBAHAYkj8AAA5D8gcAwGH+Pz9V6feOnmw3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62909a",
   "metadata": {},
   "source": [
    "got accuracy of 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

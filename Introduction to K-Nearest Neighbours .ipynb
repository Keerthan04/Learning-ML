{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55343105",
   "metadata": {},
   "source": [
    "# Introduction to K-Nearest Neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b570407",
   "metadata": {},
   "source": [
    "# Representing Points"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6d2791a",
   "metadata": {},
   "source": [
    "Representing Points\n",
    "In this lesson, you will learn three different ways to define the distance between two points:\n",
    "\n",
    "Euclidean Distance\n",
    "Manhattan Distance\n",
    "Hamming Distance\n",
    "Before diving into the distance formulas, it is first important to consider how to represent points in your code.\n",
    "\n",
    "In this exercise, we will use a list, where each item in the list represents a dimension of the point. For example, the point (5, 8) could be represented in Python like this:\n",
    "\n",
    "pt1 = [5, 8]\n",
    "\n",
    "Points aren’t limited to just two dimensions. For example, a five-dimensional point could be represented as [4, 8, 15, 16, 23].\n",
    "\n",
    "Ultimately, we want to find the distance between two points. We’ll be writing functions that look like this:\n",
    "\n",
    "distance([1, 2, 3], [5, 8, 9])\n",
    "\n",
    "Note that we can only find the difference between two points if they have the same number of dimensions!\n",
    "\n",
    "Example of points\n",
    "two_d = [10, 2]\n",
    "five_d = [30, -1, 50, 0, 2]\n",
    "four_d=[10,-2,5,-25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b78e60",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6658a9d2",
   "metadata": {},
   "source": [
    "Euclidean Distance\n",
    "Euclidean Distance is the most commonly used distance formula. To find the Euclidean distance between two points, we first calculate the squared distance between each dimension. If we add up all of these squared differences and take the square root, we’ve computed the Euclidean distance.\n",
    "\n",
    "Let’s take a look at the equation that represents what we just learned:\n",
    " d = sqrt( (x2-x1)^2 + (y2-y1)^2 + (z2-z1)^2 )\n",
    " \n",
    "euclidean distance function defining:\n",
    "\n",
    "def euclidean_distance(pt1, pt2):\n",
    "    distance = 0\n",
    "    for i in range(len(pt1)):\n",
    "        distance += (pt1[i] - pt2[i]) ** 2\n",
    "    return distance ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea4b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.605551275463989\n",
      "7.810249675906654\n"
     ]
    }
   ],
   "source": [
    "#example of euclidean distance\n",
    "def euclidean_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    distance += (pt1[i] - pt2[i]) ** 2\n",
    "  return distance ** 0.5\n",
    "\n",
    "print(euclidean_distance([1, 2], [4, 0]))\n",
    "print(euclidean_distance([5, 4, 3], [1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca39d4",
   "metadata": {},
   "source": [
    "## Manhattan Distance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7126cd0c",
   "metadata": {},
   "source": [
    "Manhattan Distance\n",
    "Manhattan Distance is extremely similar to Euclidean distance. Rather than summing the squared difference between each dimension, we instead sum the absolute value of the difference between each dimension. It’s called Manhattan distance because it’s similar to how you might navigate when walking city blocks. If you’ve ever wondered “how many blocks will it take me to get from point A to point B”, you’ve computed the Manhattan distance.\n",
    "\n",
    "The equation is shown below:\n",
    "    d=|x1-x2|+|y1-y2|+|z1-z2|\n",
    "Note that Manhattan distance will always be greater than or equal to Euclidean distance.\n",
    "\n",
    "manhattan distance function building\n",
    "def manhattan_distance(pt1,pt2):\n",
    "  distance=0;\n",
    "  for i in range(len(pt1)):\n",
    "    distance += abs(pt1[i] - pt2[i])\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc733e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#example of manhattan distance\n",
    "def manhattan_distance(pt1,pt2):\n",
    "  distance=0;\n",
    "  for i in range(len(pt1)):\n",
    "    distance += abs(pt1[i] - pt2[i])\n",
    "  return distance\n",
    "\n",
    "print(manhattan_distance([1, 2], [4, 0]))\n",
    "print(manhattan_distance([5, 4, 3], [1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380532f2",
   "metadata": {},
   "source": [
    "## Hamming Distance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee0cb21e",
   "metadata": {},
   "source": [
    "Hamming Distance\n",
    "Hamming Distance is another slightly different variation on the distance formula. Instead of finding the difference of each dimension, Hamming distance only cares about whether the dimensions are exactly equal. When finding the Hamming distance between two points, add one for every dimension that has different values.\n",
    "\n",
    "Hamming distance is used in spell checking algorithms. For example, the Hamming distance between the word “there” and the typo “thete” is one. Each letter is a dimension, and each dimension has the same value except for one.\n",
    "\n",
    "hamming distnce logic building\n",
    "\n",
    "def hamming_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    if pt1[i] != pt2[i]:\n",
    "      distance += 1\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0b29bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#example of hamming distance\n",
    "def hamming_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    if pt1[i] != pt2[i]:\n",
    "      distance += 1\n",
    "  return distance\n",
    "\n",
    "print(hamming_distance([1, 2], [1, 100]))\n",
    "print(hamming_distance([5, 4, 9], [1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bb371",
   "metadata": {},
   "source": [
    "## Scipy library-distance calculation function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55bab0e8",
   "metadata": {},
   "source": [
    "SciPy Distances\n",
    "Now that you’ve written these three distance formulas yourself, let’s look at how to use them using Python’s SciPy library:\n",
    "\n",
    "Euclidean Distance .euclidean()\n",
    "Manhattan Distance .cityblock()\n",
    "Hamming Distance .hamming()\n",
    "There are a few noteworthy details to talk about:\n",
    "\n",
    "First, the scipy implementation of Manhattan distance is called cityblock(). Remember, computing Manhattan distance is like asking how many blocks away you are from a point.\n",
    "\n",
    "Second, the scipy implementation of Hamming distance will always return a number between 0 an 1. Rather than summing the number of differences in dimensions, this implementation sums those differences and then divides by the total number of dimensions. For example, in your implementation, the Hamming distance between [1, 2, 3] and [7, 2, -10] would be 2. In scipy‘s version, it would be 2/3.\n",
    "\n",
    "importing:\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae40015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.605551275463989\n",
      "5\n",
      "2\n",
      "3.605551275463989\n",
      "5\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#example of both ours and scipy lib\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def euclidean_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    distance += (pt1[i] - pt2[i]) ** 2\n",
    "  return distance ** 0.5\n",
    "\n",
    "def manhattan_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    distance += abs(pt1[i] - pt2[i])\n",
    "  return distance\n",
    "\n",
    "def hamming_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    if pt1[i] != pt2[i]:\n",
    "      distance += 1\n",
    "  return distance\n",
    "\n",
    "print(euclidean_distance([1, 2], [4, 0]))\n",
    "print(manhattan_distance([1, 2], [4, 0]))\n",
    "print(hamming_distance([5, 4, 9], [1, 7, 9]))\n",
    "\n",
    "print(distance.euclidean([1, 2], [4, 0]))\n",
    "print(distance.cityblock([1, 2], [4, 0]))\n",
    "print(distance.hamming([5, 4, 9], [1, 7, 9]))#here we see difference in answer from scipy lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521c911",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f65577e9",
   "metadata": {},
   "source": [
    "    Normalization\n",
    "This article describes why normalization is necessary. It also demonstrates the pros and cons of min-max normalization and z-score normalization.\n",
    "\n",
    "    Why Normalize?\n",
    "Many machine learning algorithms attempt to find trends in the data by comparing features of data points. However, there is an issue when the features are on drastically different scales.\n",
    "\n",
    "For example, consider a dataset of houses. Two potential features might be the number of rooms in the house, and the total age of the house in years. A machine learning algorithm could try to predict which house would be best for you. However, when the algorithm compares data points, the feature with the larger scale will completely dominate the other. Take a look at the image below:\n",
    "\n",
    "Data points on the y-axis range from 0 to 20. Data points on the x-axis range from 0 to 100\n",
    "\n",
    "When the data looks squished like that, we know we have a problem. The machine learning algorithm should realize that there is a huge difference between a house with 2 rooms and a house with 20 rooms. But right now, because two houses can be 100 years apart, the difference in the number of rooms contributes less to the overall difference.\n",
    "\n",
    "As a more extreme example, imagine what the graph would look like if the x-axis was the cost of the house. The data would look even more squished; the difference in the number of rooms would be even less relevant because the cost of two houses could have a difference of thousands of dollars.\n",
    "\n",
    "The goal of normalization is to make every datapoint have the same scale so each feature is equally important. The image below shows the same house data normalized using min-max normalization.\n",
    "\n",
    "Data points on the y-axis range from 0 to 1. Data points on the x-axis range from 0 to 1\n",
    "\n",
    "Min-Max Normalization\n",
    "Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n",
    "\n",
    "For example, if the minimum value of a feature was 20, and the maximum value was 40, then 30 would be transformed to about 0.5 since it is halfway between 20 and 40. The formula is as follows:\n",
    "\n",
    "    new_value = (value - min)/(max - min)\n",
    " \n",
    "Min-max normalization has one fairly significant downside: it does not handle outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before! Take a look at the image below to see an example of this.\n",
    "\n",
    "Almost all normalized data points have an x value between 0 and 0.4\n",
    "\n",
    "Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.\n",
    "\n",
    "Z-Score Normalization\n",
    "Z-score normalization is a strategy of normalizing data that avoids this outlier issue. The formula for Z-score normalization is below:\n",
    "\n",
    "    new_value(z) = (value - μ)/σ\n",
    " \n",
    "Here, μ is the mean value of the feature and σ is the standard deviation of the feature. If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature. If the unnormalized data had a large standard deviation, the normalized values will be closer to 0.\n",
    "\n",
    "Take a look at the graph below. This is the same data as before, but this time we’re using z-score normalization.\n",
    "\n",
    "All points have a similar range in both the x and y dimensions\n",
    "\n",
    "While the data still looks squished, notice that the points are now on roughly the same scale for both features — almost all points are between -2 and 2 on both the x-axis and y-axis. The only potential downside is that the features aren’t on the exact same scale.\n",
    "\n",
    "With min-max normalization, we were guaranteed to reshape both of our features to be between 0 and 1. Using z-score normalization, the x-axis now has a range from about -1.5 to 1.5 while the y-axis has a range from about -2 to 2. This is certainly better than before; the x-axis, which previously had a range of 0 to 40, is no longer dominating the y-axis.\n",
    "\n",
    "    Review\n",
    "Normalizing your data is an essential part of machine learning. You might have an amazing dataset with many great features, but if you forget to normalize, one of those features might completely dominate the others. It’s like you’re throwing away almost all of your information! Normalizing solves this problem. In this article, you learned the following techniques to normalize:\n",
    "\n",
    "Min-max normalization: Guarantees all features will have the exact same scale but does not handle outliers well.\n",
    "Z-score normalization: Handles outliers, but does not produce normalized data with the exact same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30fcf4",
   "metadata": {},
   "source": [
    "## Training set vs validating set vs test set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f065ae5",
   "metadata": {},
   "source": [
    "Training Set vs Validation Set vs Test Set\n",
    "This article teaches the importance of splitting a data set into training, validation and test sets.\n",
    "\n",
    "    Testing Our Model\n",
    "Supervised machine learning algorithms are amazing tools capable of making predictions and classifications. However, it is important to ask yourself how accurate those predictions are. After all, it’s possible that every prediction your classifier makes is actually wrong! Luckily, we can leverage the fact that supervised machine learning algorithms, by definition, have a dataset of pre-labeled datapoints. In order to test the effectiveness of your algorithm, we’ll split this data into:\n",
    "        training set\n",
    "        validation set\n",
    "        test set\n",
    "    Training Set vs Validation Set\n",
    "The training set is the data that the algorithm will learn from. Learning looks different depending on which algorithm you are using. For example, when using Linear Regression, the points in the training set are used to draw the line of best fit. In K-Nearest Neighbors, the points in the training set are the points that could be the neighbors.\n",
    "\n",
    "After training using the training set, the points in the validation set are used to compute the accuracy or error of the classifier. The key insight here is that we know the true labels of every point in the validation set, but we’re temporarily going to pretend like we don’t. We can use every point in the validation set as input to our classifier. We’ll then receive a classification for that point. We can now peek at the true label of the validation point and see whether we got it right or not. If we do this for every point in the validation set, we can compute the validation error!\n",
    "\n",
    "Validation error might not be the only metric we’re interested in. A better way of judging the effectiveness of a machine learning algorithm is to compute its precision, recall, and F1 score.\n",
    "\n",
    "    How to Split\n",
    "Figuring out how much of your data should be split into your validation set is a tricky question. If your training set is too small, then your algorithm might not have enough data to effectively learn. On the other hand, if your validation set is too small, then your accuracy, precision, recall, and F1 score could have a large variance. You might happen to get a really lucky or a really unlucky split! In general, putting 80% of your data in the training set, and 20% of your data in the validation set is a good place to start.\n",
    "\n",
    "    N-Fold Cross-Validation\n",
    "Sometimes your dataset is so small, that splitting it 80/20 will still result in a large amount of variance. One solution to this is to perform N-Fold Cross-Validation. The central idea here is that we’re going to do this entire process N times and average the accuracy. For example, in 10-fold cross-validation, we’ll make the validation set the first 10% of the data and calculate accuracy, precision, recall and F1 score. We’ll then make the validation set the second 10% of the data and calculate these statistics again. We can do this process 10 times, and every time the validation set will be a different chunk of the data. If we then average all of the accuracies, we will have a better sense of how our model does on average.\n",
    "\n",
    "    Changing The Model / Test Set\n",
    "Understanding the accuracy of your model is invaluable because you can begin to tune the parameters of your model to increase its performance. For example, in the K-Nearest Neighbors algorithm, you can watch what happens to accuracy as you increase or decrease K. (You can try out all of this in our K-Nearest Neighbors lesson!)\n",
    "\n",
    "Once you’re happy with your model’s performance, it is time to introduce the test set. This is part of your data that you partitioned away at the very start of your experiment. It’s meant to be a substitute for the data in the real world that you’re actually interested in classifying. It functions very similarly to the validation set, except you never touched this data while building or tuning your model. By finding the accuracy, precision, recall, and F1 score on the test set, you get a good understanding of how well your algorithm will do in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79e0c4",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6d56114",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Classifier\n",
    "K-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories.\n",
    "\n",
    "Consider the image to the right.(img in folder) This image is complicated, but for now, let’s just focus on where the data points are being placed. Every data point — whether its color is red, green, or white — has an x value and a y value. As a result, it can be plotted on this two-dimensional graph.\n",
    "\n",
    "Next, let’s consider the color of the data. The color represents the class that the K-Nearest Neighbor algorithm is trying to classify. In this image, data points can either have the class green or the class red. If a data point is white, this means that it doesn’t have a class yet. The purpose of the algorithm is to classify these unknown points.\n",
    "\n",
    "Finally, consider the expanding circle around the white point. This circle is finding the k nearest neighbors to the white point. When k = 3, the circle is fairly small. Two of the three nearest neighbors are green, and one is red. So in this case, the algorithm would classify the white point as green. However, when we increase k to 5, the circle expands, and the classification changes. Three of the nearest neighbors are red and two are green, so now the white point will be classified as red.\n",
    "\n",
    "This is the central idea behind the K-Nearest Neighbor algorithm. If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it’s nearest neighbors, and classify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8f51f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2adc8a3",
   "metadata": {},
   "source": [
    "Introduction\n",
    "Before diving into the K-Nearest Neighbors algorithm, let’s first take a minute to think about an example.\n",
    "\n",
    "Consider a dataset of movies. Let’s brainstorm some features of a movie data point. A feature is a piece of information associated with a data point. Here are some potential features of movie data points:\n",
    "\n",
    "the length of the movie in minutes.\n",
    "the budget of a movie in dollars.\n",
    "If you think back to the previous exercise, you could imagine movies being places in that two-dimensional space based on those numeric features. There could also be some boolean features: features that are either true or false. For example, here are some potential boolean features:\n",
    "\n",
    "Black and white. This feature would be True for black and white movies and False otherwise.\n",
    "\n",
    "Directed by Stanley Kubrick. This feature would be False for almost every movie, but for the few movies that were directed by Kubrick, it would be True.\n",
    "\n",
    "Finally, let’s think about how we might want to classify a movie. For the rest of this lesson, we’re going to be classifying movies as either good or bad. In our dataset, we’ve classified a movie as good if it had an IMDb rating of 7.0 or greater. Every “good” movie will have a class of 1, while every bad movie will have a class of 0.\n",
    "\n",
    "To the right, we’ve created some movie data points where the first item in the list is the length, the second is the budget, and the third is whether the movie was directed by Stanley Kubrick.\n",
    "\n",
    "ex\n",
    "mean_girls = [97, 17000000, False]\n",
    "the_shining = [146, 19000000, True]\n",
    "gone_with_the_wind= [238, 3977000, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a493c",
   "metadata": {},
   "source": [
    "## Distance between 2 points in 2D space"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e31389b",
   "metadata": {},
   "source": [
    "Distance Between Points - 2D\n",
    "In the first exercise, we were able to visualize the dataset and estimate the k nearest neighbors of an unknown point. But a computer isn’t going to be able to do that!\n",
    "\n",
    "We need to define what it means for two points to be close together or far apart. To do this, we’re going to use the Distance Formula.\n",
    "\n",
    "For this example, the data has two dimensions:\n",
    "\n",
    "The length of the movie\n",
    "The movie’s release date\n",
    "Consider Star Wars and Raiders of the Lost Ark. Star Wars is 125 minutes long and was released in 1977. Raiders of the Lost Ark is 115 minutes long and was released in 1981.\n",
    "\n",
    "The distance between the movies is computed below:\n",
    "\n",
    "Distance = √((x2 - x1)2 + (y2 - y1)2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6001775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.770329614269007\n",
      "38.897300677553446\n"
     ]
    }
   ],
   "source": [
    "#example of classification in 2d\n",
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "def distance(movie1, movie2):#normal euvlidean distance found \n",
    "  length_difference = (movie1[0] - movie2[0]) ** 2\n",
    "  year_difference = (movie1[1] - movie2[1]) ** 2\n",
    "  distance = (length_difference + year_difference) ** 0.5\n",
    "  return distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))#more similar stars wars will be to most closer it is in distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495ef16",
   "metadata": {},
   "source": [
    "## Distance between 2 points in 3D space"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd1892e1",
   "metadata": {},
   "source": [
    "Distance Between Points - 3D\n",
    "Making a movie rating predictor based on just the length and release date of movies is pretty limited. There are so many more interesting pieces of data about movies that we could use! So let’s add another dimension.\n",
    "\n",
    "Let’s say this third dimension is the movie’s budget. We now have to find the distance between these two points in three dimensions.\n",
    "\n",
    "3D graph\n",
    "What if we’re not happy with just three dimensions? Unfortunately, it becomes pretty difficult to visualize points in dimensions higher than 3. But that doesn’t mean we can’t find the distance between them.\n",
    "\n",
    "The generalized distance formula between points A and B is as follows:\n",
    " d=sqrt((A1-B1)**2+(A2-B2)**2+(A3-B3)**2)\n",
    " \n",
    "Here, A1-B1 is the difference between the first feature of each point. An-Bn is the difference between the last feature of each point.\n",
    "\n",
    "Using this formula, we can find the K-Nearest Neighbors of a point in N-dimensional space! We now can use as much information about our movies as we want.\n",
    "\n",
    "We will eventually use these distances to find the nearest neighbors to an unlabeled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8640376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000000.000008286\n",
      "6000000.000126083\n"
     ]
    }
   ],
   "source": [
    "#example of classification in 3d\n",
    "star_wars = [125, 1977, 11000000]\n",
    "raiders = [115, 1981, 18000000]\n",
    "mean_girls = [97, 2004, 17000000]\n",
    "\n",
    "def distance(movie1, movie2):#(this formula can be used for any number of dimension)\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))#so again taking any number of dimesnion we can find the distance between them and classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f1507",
   "metadata": {},
   "source": [
    "## step1-Normalize the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d160ae2",
   "metadata": {},
   "source": [
    "Data with Different Scales: Normalization\n",
    "In the next three lessons, we’ll implement the three steps of the K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "When we added the dimension of budget, you might have realized there are some problems with the way our data currently looks.\n",
    "\n",
    "Consider the two dimensions of release date and budget. The maximum difference between two movies’ release dates is about 125 years (The Lumière Brothers were making movies in the 1890s). However, the difference between two movies’ budget can be millions of dollars.\n",
    "\n",
    "The problem is that the distance formula treats all dimensions equally, regardless of their scale. If two movies came out 70 years apart, that should be a pretty big deal. However, right now, that’s exactly equivalent to two movies that have a difference in budget of 70 dollars. The difference in one year is exactly equal to the difference in one dollar of budget. That’s absurd!\n",
    "\n",
    "Another way of thinking about this is that the budget completely outweighs the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 70 years apart is essentially meaningless compared to the difference in millions in the other dimension.\n",
    "\n",
    "The solution to this problem is to normalize the data so every value is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a3b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n"
     ]
    }
   ],
   "source": [
    "#example of using normalizing of data using min_max_nomalize\n",
    "release_dates = [1897.0, 1998.0, 2000.0, 1948.0, 1962.0, 1950.0, 1975.0, 1960.0, 2017.0, 1937.0, 1968.0, 1996.0, 1944.0, 1891.0, 1995.0, 1948.0, 2011.0, 1965.0, 1891.0, 1978.0]\n",
    "#for our function the data type is float of all \n",
    "def min_max_normalize(lst):\n",
    "  minimum = min(lst)\n",
    "  maximum = max(lst)\n",
    "  normalized = []\n",
    "  \n",
    "  for value in lst:\n",
    "    normalized_num = (value - minimum) / (maximum - minimum)\n",
    "    normalized.append(normalized_num)\n",
    "  \n",
    "  return normalized\n",
    "\n",
    "print(min_max_normalize(release_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d567b01",
   "metadata": {},
   "source": [
    "## Step-2-Finding the Nearest Neighbour"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e20708b7",
   "metadata": {},
   "source": [
    "Finding the Nearest Neighbors\n",
    "The K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!\n",
    "\n",
    "To do this, we want to find the k nearest neighbors of the unclassified point. In a few exercises, we’ll learn how to properly choose k, but for now, let’s choose a number that seems somewhat reasonable. Let’s choose 5.\n",
    "\n",
    "In order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.\n",
    "\n",
    "It might look something like this:\n",
    "\n",
    "[\n",
    "  [0.30, 'Superman II'],\n",
    "  [0.31, 'Finding Nemo'],\n",
    "  ...\n",
    "  ...\n",
    "  [0.38, 'Blazing Saddles']\n",
    "]\n",
    "\n",
    "In this example, the unknown movie has a distance of 0.30 to Superman II.\n",
    "\n",
    "In the next exercise, we’ll use the labels associated with these movies to classify the unlabeled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dee8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, movie_labels\n",
    "\n",
    "#print(movie_dataset['Bruce Almighty'])\n",
    "#print(movie_labels['Bruce Almighty'])\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, k):#k is the number of nearest neighbors(discussed further)\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  return neighbors\n",
    "  \n",
    "print(classify([.4, .2, .9], movie_dataset, 5))\n",
    "#so what we do is we pass the unknown ka to function and we pass the dataset and k,then each distance is calculated with all movies and appended to distances list with the distance and also the name then we sort and return only k movies so when we pass any point we get the closest k movies to that point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output will be\n",
    "Output:\n",
    "[[0.08273614694606074, 'Lady Vengeance'], \n",
    " [0.22989623153818367, 'Steamboy'], [0.23641372358159884, 'Fateless'],\n",
    " [0.26735445689589943, 'Princess Mononoke'],\n",
    " [0.3311022951533416, 'Godzilla 2000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc70206",
   "metadata": {},
   "source": [
    "## Step-3-Classifying by counting the nearest neighbors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8948a1d2",
   "metadata": {},
   "source": [
    "Count Neighbors\n",
    "The K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "We’ve now found the k nearest neighbors, and have stored them in a list that looks like this:\n",
    "\n",
    "\n",
    "Explain\n",
    "[\n",
    "  [0.083, 'Lady Vengeance'],\n",
    "  [0.236, 'Steamboy'],\n",
    "  ...\n",
    "  ...\n",
    "  [0.331, 'Godzilla 2000']\n",
    "]\n",
    "\n",
    "Our goal now is to count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad.\n",
    "\n",
    "In order to find the class of each of the labels, we’ll need to look at our movie_labels dataset. For example, movie_labels['Akira'] would give us 1 because Akira is classified as a good movie.\n",
    "\n",
    "You may be wondering what happens if there’s a tie. What if k = 8 and four neighbors were good and four neighbors were bad? There are different strategies, but one way to break the tie would be to choose the class of the closest point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete code for classification\n",
    "from movies import movie_dataset, movie_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset,labels, k):\n",
    "  distances = []\n",
    "  \n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for movie in neighbors:\n",
    "    title=movie[1]\n",
    "    if labels[title]==0:\n",
    "      num_bad+=1\n",
    "    else:\n",
    "      num_good+=1\n",
    "    if(num_good > num_bad):\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "print(classify([.4, .2, .9],movie_dataset,movie_labels,5))#output is 1(good)\n",
    "#so once neighbors got we take the title of each neigbour and check if it is good or bad from labels and increment count of good and bad and finally based on good and bad we classify if tie(we can see first movi ine neighbour which is nearest ka good or bad and classify based on that)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eed8a13e",
   "metadata": {},
   "source": [
    "Classify Your Favorite Movie\n",
    "Nice work! Your classifier is now able to predict whether a movie will be good or bad. So far, we’ve only tested this on a completely random point [.4, .2, .9]. In this exercise we’re going to pick a real movie, normalize it, and run it through our classifier to see what it predicts!\n",
    "\n",
    "In the instructions below, we are going to be testing our classifier using the 2017 movie Call Me By Your Name. Feel free to pick your favorite movie instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50563900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, movie_labels, normalize_point\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "#print(\"Call Me By Your Name\" in movie_dataset)\n",
    "my_movie=[3500000, 132, 2017]\n",
    "normalized_my_movie=normalize_point(my_movie)\n",
    "print(classify(normalized_my_movie, movie_dataset, movie_labels, 5))#output is 1(good)\n",
    "#so to make our own movie we first see in datset(if there on neigbour itself so shd not) once not then make a list of all for function and normalize using function and see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ffc51",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75e8de6f",
   "metadata": {},
   "source": [
    "Training and Validation Sets\n",
    "You’ve now built your first K Nearest Neighbors algorithm capable of classification. You can feed your program a never-before-seen movie and it can predict whether its IMDb rating was above or below 7.0. However, we’re not done yet. We now need to report how effective our algorithm is. After all, it’s possible our predictions are totally wrong!\n",
    "\n",
    "As with most machine learning algorithms, we have split our data into a training set and validation set.\n",
    "\n",
    "Once these sets are created, we will want to use every point in the validation set as input to the K Nearest Neighbor algorithm. We will take a movie from the validation set, compare it to all the movies in the training set, find the K Nearest Neighbors, and make a prediction. After making that prediction, we can then peek at the real answer (found in the validation labels) to see if our classifier got the answer correct.\n",
    "\n",
    "If we do this for every movie in the validation set, we can count the number of times the classifier got the answer right and the number of times it got it wrong. Using those two numbers, we can compute the validation accuracy.\n",
    "\n",
    "Validation accuracy will change depending on what K we use. In the next exercise, we’ll use the validation accuracy to pick the best possible K for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing it for only one movie from validation set and checking\n",
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "#print(validation_set[\"Bee Movie\"])\n",
    "#print(validation_labels[\"Bee Movie\"])\n",
    "guess=classify(validation_set[\"Bee Movie\"],training_set,training_labels,5)\n",
    "if guess==validation_labels[\"Bee Movie\"]:\n",
    "  print(\"Correct!\")\n",
    "else:\n",
    "  print(\"Wrong\")#output is correct(means both prediction and actual are same)\n",
    "#like this we need to do for complete validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fc321",
   "metadata": {},
   "source": [
    "## Choosing k"
   ]
  },
  {
   "cell_type": "raw",
   "id": "220e9f54",
   "metadata": {},
   "source": [
    "Choosing K\n",
    "In the previous exercise, we found that our classifier got one point in the training set correct. Now we can test every point to calculate the validation accuracy.\n",
    "\n",
    "The validation accuracy changes as k changes. The first situation that will be useful to consider is when k is very small. Let’s say k = 1. We would expect the validation accuracy to be fairly low due to overfitting. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point. Consider the image below.(in folder)\n",
    "\n",
    "colored dots with a single outlier\n",
    "\n",
    "The dark blue point in the top left corner of the graph looks like a fairly significant outlier. When k = 1, all points in that general area will be classified as dark blue when it should probably be classified as green. Our classifier has relied too heavily on the small quirks in the training data.\n",
    "\n",
    "On the other hand, if k is very large, our classifier will suffer from underfitting. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set. Imagine you have 100 points in your training set and you set k = 100. Every single unknown point will be classified in the same exact way. The distances between the points don’t matter at all! This is an extreme example, however, it demonstrates how the classifier can lose understanding of the training data if k is too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to validating accuracy\n",
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):\n",
    "  num_correct = 0.0\n",
    "  for title in validation_set:\n",
    "    guess = classify(validation_set[title], training_set, training_labels, k)\n",
    "    if guess == validation_labels[title]:\n",
    "      num_correct += 1\n",
    "  return num_correct / len(validation_set)\n",
    "\n",
    "\n",
    "print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, 3))#output is 66%(0.66)\n",
    "#so in validating for each movie in validation_set we guess from our function and check if it is correct or not and if correct adding and finally getting the accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2f4c4d0",
   "metadata": {},
   "source": [
    "Graph of K\n",
    "The graph to the right(img folder) shows the validation accuracy of our movie classifier as k increases. When k is small, overfitting occurs and the accuracy is relatively low. On the other hand, when k gets too large, underfitting occurs and accuracy starts to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a344174",
   "metadata": {},
   "source": [
    "# Using Sklearn for KNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccd1ddaa",
   "metadata": {},
   "source": [
    "Using sklearn\n",
    "You’ve now written your own K-Nearest Neighbor classifier from scratch! However, rather than writing your own classifier every time, you can use Python’s sklearn library. sklearn is a Python library specifically used for Machine Learning. It has an amazing number of features, but for now, we’re only going to investigate its K-Nearest Neighbor classifier.\n",
    "\n",
    "There are a couple of steps we’ll need to go through in order to use the library. First, you need to create a KNeighborsClassifier object. This object takes one parameter - k. For example, the code below will create a classifier where k = 3\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "Next, we’ll need to train our classifier. The .fit() method takes two parameters. The first is a list of points, and the second is the labels associated with those points. So for our movie example, we might have something like this\n",
    "\n",
    "\n",
    "training_points = [\n",
    "  [0.5, 0.2, 0.1],\n",
    "  [0.9, 0.7, 0.3],\n",
    "  [0.4, 0.5, 0.7]\n",
    "]\n",
    "\n",
    "training_labels = [0, 1, 1]\n",
    "classifier.fit(training_points, training_labels)\n",
    "\n",
    "Finally, after training the model, we can classify new points. The .predict() method takes a list of points that you want to classify. It returns a list of its guesses for those points.\n",
    "\n",
    "\n",
    "unknown_points = [\n",
    "  [0.2, 0.1, 0.7],\n",
    "  [0.4, 0.7, 0.6],\n",
    "  [0.5, 0.8, 0.1]\n",
    "]\n",
    "\n",
    "guesses = classifier.predict(unknown_points)\n",
    "\n",
    "importing is:\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how to do using sklearn\n",
    "from movies import movie_dataset, labels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier.fit(movie_dataset, labels)\n",
    "guess = classifier.predict([[.45, .2, .5], [.25, .8, .9],[.1, .1, .9]])#see how passed when predicting\n",
    "print(guess)#got [1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee716",
   "metadata": {},
   "source": [
    "!!!IMPORTANT!!\n",
    "Learn how accuracy of this and all checking\n",
    "see how to do it\n",
    "accuracy_score and others can we put and all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2481d69b",
   "metadata": {},
   "source": [
    "Some of the major takeaways from this lesson include:\n",
    "\n",
    "Data with n features can be conceptualized as points lying in n-dimensional space.\n",
    "Data points can be compared by using the distance formula. Data points that are similar will have a smaller distance between them.\n",
    "A point with an unknown class can be classified by finding the k nearest neighbors\n",
    "To verify the effectiveness of a classifier, data with known classes can be split into a training set and a validation set. Validation error can then be calculated.\n",
    "Classifiers have parameters that can be tuned to increase their effectiveness. In the case of K-Nearest Neighbors, k can be changed.\n",
    "A classifier can be trained improperly and suffer from overfitting or underfitting. In the case of K-Nearest Neighbors, a low k often leads to overfitting and a large k often leads to underfitting.\n",
    "Python’s sklearn library can be used for many classification and machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
